{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Certain functionality \n",
      "             requires requests_html, which is not installed.\n",
      "             \n",
      "             Install using: \n",
      "             pip install requests_html\n",
      "             \n",
      "             After installation, you may have to restart your Python session.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "### model parameters\n",
    "\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "### training parameters\n",
    "\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500\n",
    "\n",
    "# Amazon stock market\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "\n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    \n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    \n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "\n",
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.0024 - mean_absolute_error: 0.0290\n",
      "Epoch 1: val_loss improved from inf to 0.00045, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 34s 351ms/step - loss: 0.0024 - mean_absolute_error: 0.0290 - val_loss: 4.4579e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 2/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.9864e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 2: val_loss improved from 0.00045 to 0.00039, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 31s 360ms/step - loss: 6.9864e-04 - mean_absolute_error: 0.0186 - val_loss: 3.8841e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 3/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 7.6203e-04 - mean_absolute_error: 0.0200\n",
      "Epoch 3: val_loss improved from 0.00039 to 0.00038, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 31s 362ms/step - loss: 7.6203e-04 - mean_absolute_error: 0.0200 - val_loss: 3.7961e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 4/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.7058e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 4: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 30s 357ms/step - loss: 6.7058e-04 - mean_absolute_error: 0.0181 - val_loss: 4.0236e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 5/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 7.3353e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 5: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 30s 350ms/step - loss: 7.3353e-04 - mean_absolute_error: 0.0193 - val_loss: 5.6113e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 6/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 7.6526e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 6: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 32s 378ms/step - loss: 7.6526e-04 - mean_absolute_error: 0.0197 - val_loss: 3.9511e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 7/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 7.1871e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 7: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 31s 361ms/step - loss: 7.1871e-04 - mean_absolute_error: 0.0190 - val_loss: 7.5493e-04 - val_mean_absolute_error: 0.0221\n",
      "Epoch 8/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.5219e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 8: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 31s 370ms/step - loss: 6.5219e-04 - mean_absolute_error: 0.0185 - val_loss: 4.2305e-04 - val_mean_absolute_error: 0.0158\n",
      "Epoch 9/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.3798e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 9: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 35s 412ms/step - loss: 6.3798e-04 - mean_absolute_error: 0.0181 - val_loss: 3.8659e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 10/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 7.0903e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 10: val_loss did not improve from 0.00038\n",
      "85/85 [==============================] - 34s 397ms/step - loss: 7.0903e-04 - mean_absolute_error: 0.0188 - val_loss: 5.4360e-04 - val_mean_absolute_error: 0.0161\n",
      "Epoch 11/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.5555e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 11: val_loss improved from 0.00038 to 0.00036, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 35s 408ms/step - loss: 6.5555e-04 - mean_absolute_error: 0.0188 - val_loss: 3.6466e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 12/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.3760e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 12: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 6.3760e-04 - mean_absolute_error: 0.0181 - val_loss: 3.7189e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 13/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 7.3026e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 13: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 35s 410ms/step - loss: 7.3026e-04 - mean_absolute_error: 0.0198 - val_loss: 0.0012 - val_mean_absolute_error: 0.0255\n",
      "Epoch 14/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.4701e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 14: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 35s 414ms/step - loss: 6.4701e-04 - mean_absolute_error: 0.0185 - val_loss: 4.2228e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 15/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.7037e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 15: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 5.7037e-04 - mean_absolute_error: 0.0176 - val_loss: 3.8237e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 16/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.0157e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 16: val_loss improved from 0.00036 to 0.00036, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 31s 368ms/step - loss: 6.0157e-04 - mean_absolute_error: 0.0180 - val_loss: 3.5979e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 17/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.2814e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 17: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 33s 383ms/step - loss: 6.2814e-04 - mean_absolute_error: 0.0184 - val_loss: 0.0012 - val_mean_absolute_error: 0.0240\n",
      "Epoch 18/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.3955e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 18: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 6.3955e-04 - mean_absolute_error: 0.0190 - val_loss: 5.2816e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 19/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.8935e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 19: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 32s 378ms/step - loss: 5.8935e-04 - mean_absolute_error: 0.0181 - val_loss: 4.3145e-04 - val_mean_absolute_error: 0.0162\n",
      "Epoch 20/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.7897e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 20: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 32s 371ms/step - loss: 5.7897e-04 - mean_absolute_error: 0.0179 - val_loss: 3.6639e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 21/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.4972e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 21: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 33s 387ms/step - loss: 5.4972e-04 - mean_absolute_error: 0.0175 - val_loss: 3.7147e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 22/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.5861e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 22: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 34s 397ms/step - loss: 5.5861e-04 - mean_absolute_error: 0.0176 - val_loss: 4.0956e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 23/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.6976e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 23: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 34s 406ms/step - loss: 5.6976e-04 - mean_absolute_error: 0.0178 - val_loss: 6.0536e-04 - val_mean_absolute_error: 0.0176\n",
      "Epoch 24/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.0983e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 24: val_loss did not improve from 0.00036\n",
      "85/85 [==============================] - 35s 415ms/step - loss: 6.0983e-04 - mean_absolute_error: 0.0186 - val_loss: 4.0033e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 25/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.5737e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 25: val_loss improved from 0.00036 to 0.00035, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 35s 412ms/step - loss: 5.5737e-04 - mean_absolute_error: 0.0178 - val_loss: 3.4623e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 26/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.7820e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 26: val_loss improved from 0.00035 to 0.00034, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 34s 399ms/step - loss: 5.7820e-04 - mean_absolute_error: 0.0182 - val_loss: 3.4400e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 27/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 6.0894e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 27: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 35s 418ms/step - loss: 6.0894e-04 - mean_absolute_error: 0.0185 - val_loss: 4.7205e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 28/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.7241e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 28: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 35s 416ms/step - loss: 5.7241e-04 - mean_absolute_error: 0.0182 - val_loss: 4.1004e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 29/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.5239e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 29: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 37s 433ms/step - loss: 5.5239e-04 - mean_absolute_error: 0.0180 - val_loss: 3.8877e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 30/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.4341e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 30: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 34s 394ms/step - loss: 5.4341e-04 - mean_absolute_error: 0.0179 - val_loss: 5.3177e-04 - val_mean_absolute_error: 0.0183\n",
      "Epoch 31/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.8524e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 31: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 34s 396ms/step - loss: 5.8524e-04 - mean_absolute_error: 0.0188 - val_loss: 3.5152e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 32/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.8606e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 32: val_loss improved from 0.00034 to 0.00034, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 5.8606e-04 - mean_absolute_error: 0.0182 - val_loss: 3.3899e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 33/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3982e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 33: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 5.3982e-04 - mean_absolute_error: 0.0183 - val_loss: 3.7339e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 34/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.8668e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 34: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 34s 398ms/step - loss: 5.8668e-04 - mean_absolute_error: 0.0189 - val_loss: 3.7393e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 35/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3931e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 35: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 33s 389ms/step - loss: 5.3931e-04 - mean_absolute_error: 0.0182 - val_loss: 3.4698e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 36/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2707e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 36: val_loss improved from 0.00034 to 0.00034, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 34s 396ms/step - loss: 5.2707e-04 - mean_absolute_error: 0.0181 - val_loss: 3.3890e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 37/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.6790e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 37: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 5.6790e-04 - mean_absolute_error: 0.0186 - val_loss: 5.1452e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 38/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.5920e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 38: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 33s 394ms/step - loss: 5.5920e-04 - mean_absolute_error: 0.0184 - val_loss: 3.3946e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 39/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.5742e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 39: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 5.5742e-04 - mean_absolute_error: 0.0186 - val_loss: 3.4189e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 40/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.8977e-04 - mean_absolute_error: 0.0193\n",
      "Epoch 40: val_loss improved from 0.00034 to 0.00034, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 34s 401ms/step - loss: 5.8977e-04 - mean_absolute_error: 0.0193 - val_loss: 3.3829e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 41/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2717e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 41: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 5.2717e-04 - mean_absolute_error: 0.0183 - val_loss: 3.5989e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 42/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3015e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 42: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 35s 411ms/step - loss: 5.3015e-04 - mean_absolute_error: 0.0184 - val_loss: 5.3828e-04 - val_mean_absolute_error: 0.0178\n",
      "Epoch 43/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.4170e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 43: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 35s 412ms/step - loss: 5.4170e-04 - mean_absolute_error: 0.0186 - val_loss: 3.5546e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 44/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0679e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 44: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 5.0679e-04 - mean_absolute_error: 0.0180 - val_loss: 3.4279e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 45/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1993e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 45: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 5.1993e-04 - mean_absolute_error: 0.0185 - val_loss: 5.2289e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 46/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2512e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 46: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 5.2512e-04 - mean_absolute_error: 0.0187 - val_loss: 4.0206e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 47/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9490e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 47: val_loss improved from 0.00034 to 0.00034, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 34s 396ms/step - loss: 4.9490e-04 - mean_absolute_error: 0.0183 - val_loss: 3.3775e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 48/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2525e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 48: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 32s 381ms/step - loss: 5.2525e-04 - mean_absolute_error: 0.0183 - val_loss: 3.3828e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 49/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2103e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 49: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 5.2103e-04 - mean_absolute_error: 0.0185 - val_loss: 3.3847e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 50/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7721e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 50: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 33s 394ms/step - loss: 4.7721e-04 - mean_absolute_error: 0.0177 - val_loss: 3.4162e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 51/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3177e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 51: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 34s 394ms/step - loss: 5.3177e-04 - mean_absolute_error: 0.0185 - val_loss: 3.4014e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 52/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0507e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 52: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 34s 402ms/step - loss: 5.0507e-04 - mean_absolute_error: 0.0181 - val_loss: 3.6887e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 53/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1429e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 53: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 36s 429ms/step - loss: 5.1429e-04 - mean_absolute_error: 0.0182 - val_loss: 3.4707e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 54/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2228e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 54: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 35s 413ms/step - loss: 5.2228e-04 - mean_absolute_error: 0.0184 - val_loss: 3.3789e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 55/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0725e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 55: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 33s 395ms/step - loss: 5.0725e-04 - mean_absolute_error: 0.0183 - val_loss: 3.9203e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 56/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3934e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 56: val_loss did not improve from 0.00034\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 5.3934e-04 - mean_absolute_error: 0.0191 - val_loss: 3.6757e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 57/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1200e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 57: val_loss improved from 0.00034 to 0.00033, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 34s 406ms/step - loss: 5.1200e-04 - mean_absolute_error: 0.0180 - val_loss: 3.2811e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 58/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3694e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 58: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 34s 401ms/step - loss: 5.3694e-04 - mean_absolute_error: 0.0188 - val_loss: 3.3740e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 59/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7844e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 59: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.7844e-04 - mean_absolute_error: 0.0178 - val_loss: 3.7924e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 60/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9849e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 60: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.9849e-04 - mean_absolute_error: 0.0183 - val_loss: 3.5125e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 61/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1301e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 61: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 5.1301e-04 - mean_absolute_error: 0.0183 - val_loss: 5.9337e-04 - val_mean_absolute_error: 0.0187\n",
      "Epoch 62/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8892e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 62: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.8892e-04 - mean_absolute_error: 0.0181 - val_loss: 4.0642e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 63/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3962e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 63: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 5.3962e-04 - mean_absolute_error: 0.0187 - val_loss: 4.8788e-04 - val_mean_absolute_error: 0.0168\n",
      "Epoch 64/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3713e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 64: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 34s 399ms/step - loss: 5.3713e-04 - mean_absolute_error: 0.0188 - val_loss: 5.1914e-04 - val_mean_absolute_error: 0.0186\n",
      "Epoch 65/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9413e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 65: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 33s 387ms/step - loss: 4.9413e-04 - mean_absolute_error: 0.0185 - val_loss: 3.4740e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 66/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.4915e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 66: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 5.4915e-04 - mean_absolute_error: 0.0189 - val_loss: 5.7236e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 67/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9820e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 67: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 35s 415ms/step - loss: 4.9820e-04 - mean_absolute_error: 0.0183 - val_loss: 4.1001e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 68/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1419e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 68: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 5.1419e-04 - mean_absolute_error: 0.0190 - val_loss: 3.4809e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 69/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0115e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 69: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 33s 389ms/step - loss: 5.0115e-04 - mean_absolute_error: 0.0184 - val_loss: 3.5117e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 70/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9680e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 70: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.9680e-04 - mean_absolute_error: 0.0184 - val_loss: 3.3226e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 71/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1988e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 71: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 5.1988e-04 - mean_absolute_error: 0.0186 - val_loss: 4.0368e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 72/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9367e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 72: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 33s 387ms/step - loss: 4.9367e-04 - mean_absolute_error: 0.0187 - val_loss: 4.0025e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 73/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9547e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 73: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 34s 397ms/step - loss: 4.9547e-04 - mean_absolute_error: 0.0179 - val_loss: 4.1976e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 74/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1764e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 74: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 5.1764e-04 - mean_absolute_error: 0.0185 - val_loss: 3.5276e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 75/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3205e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 75: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 34s 404ms/step - loss: 5.3205e-04 - mean_absolute_error: 0.0192 - val_loss: 3.4177e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 76/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2096e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 76: val_loss did not improve from 0.00033\n",
      "85/85 [==============================] - 33s 383ms/step - loss: 5.2096e-04 - mean_absolute_error: 0.0185 - val_loss: 3.6736e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 77/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8946e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 77: val_loss improved from 0.00033 to 0.00032, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 33s 385ms/step - loss: 4.8946e-04 - mean_absolute_error: 0.0182 - val_loss: 3.2466e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 78/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7890e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 78: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 402ms/step - loss: 4.7890e-04 - mean_absolute_error: 0.0179 - val_loss: 3.8917e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 79/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9957e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 79: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 399ms/step - loss: 4.9957e-04 - mean_absolute_error: 0.0182 - val_loss: 4.0506e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 80/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3084e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 80: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 38s 443ms/step - loss: 5.3084e-04 - mean_absolute_error: 0.0192 - val_loss: 3.2807e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 81/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0393e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 81: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 409ms/step - loss: 5.0393e-04 - mean_absolute_error: 0.0184 - val_loss: 3.9133e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 82/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2085e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 82: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 5.2085e-04 - mean_absolute_error: 0.0188 - val_loss: 3.3605e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 83/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0072e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 83: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 401ms/step - loss: 5.0072e-04 - mean_absolute_error: 0.0182 - val_loss: 4.4565e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 84/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2401e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 84: val_loss improved from 0.00032 to 0.00032, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 5.2401e-04 - mean_absolute_error: 0.0190 - val_loss: 3.2360e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 85/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1734e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 85: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 389ms/step - loss: 5.1734e-04 - mean_absolute_error: 0.0188 - val_loss: 4.5739e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 86/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8159e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 86: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.8159e-04 - mean_absolute_error: 0.0181 - val_loss: 3.7235e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 87/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9227e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 87: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 4.9227e-04 - mean_absolute_error: 0.0182 - val_loss: 3.7869e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 88/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8496e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 88: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.8496e-04 - mean_absolute_error: 0.0183 - val_loss: 3.4755e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 89/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9664e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 89: val_loss improved from 0.00032 to 0.00032, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 34s 399ms/step - loss: 4.9664e-04 - mean_absolute_error: 0.0184 - val_loss: 3.2257e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 90/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7120e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 90: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.7120e-04 - mean_absolute_error: 0.0181 - val_loss: 3.2412e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 91/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0187e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 91: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 396ms/step - loss: 5.0187e-04 - mean_absolute_error: 0.0183 - val_loss: 3.2369e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 92/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1519e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 92: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 5.1519e-04 - mean_absolute_error: 0.0186 - val_loss: 3.6752e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 93/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9543e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 93: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.9543e-04 - mean_absolute_error: 0.0182 - val_loss: 3.3278e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 94/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9922e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 94: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 378ms/step - loss: 4.9922e-04 - mean_absolute_error: 0.0184 - val_loss: 3.6980e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 95/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8493e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 95: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 374ms/step - loss: 4.8493e-04 - mean_absolute_error: 0.0183 - val_loss: 3.5781e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 96/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0107e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 96: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 5.0107e-04 - mean_absolute_error: 0.0183 - val_loss: 3.7513e-04 - val_mean_absolute_error: 0.0172\n",
      "Epoch 97/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1499e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 97: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 5.1499e-04 - mean_absolute_error: 0.0189 - val_loss: 3.3871e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 98/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9294e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 98: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 373ms/step - loss: 4.9294e-04 - mean_absolute_error: 0.0179 - val_loss: 3.8025e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 99/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8224e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 99: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 377ms/step - loss: 4.8224e-04 - mean_absolute_error: 0.0184 - val_loss: 3.9822e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 100/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6468e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 100: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 4.6468e-04 - mean_absolute_error: 0.0179 - val_loss: 3.2625e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 101/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8099e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 101: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 379ms/step - loss: 4.8099e-04 - mean_absolute_error: 0.0180 - val_loss: 4.9560e-04 - val_mean_absolute_error: 0.0155\n",
      "Epoch 102/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8312e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 102: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 370ms/step - loss: 4.8312e-04 - mean_absolute_error: 0.0179 - val_loss: 3.4569e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 103/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0550e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 103: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 381ms/step - loss: 5.0550e-04 - mean_absolute_error: 0.0184 - val_loss: 3.5192e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 104/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6849e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 104: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 383ms/step - loss: 4.6849e-04 - mean_absolute_error: 0.0180 - val_loss: 3.5624e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 105/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6863e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 105: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 378ms/step - loss: 4.6863e-04 - mean_absolute_error: 0.0178 - val_loss: 3.2784e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 106/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0899e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 106: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 374ms/step - loss: 5.0899e-04 - mean_absolute_error: 0.0184 - val_loss: 3.5479e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 107/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8150e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 107: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.8150e-04 - mean_absolute_error: 0.0181 - val_loss: 3.5546e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 108/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7698e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 108: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 4.7698e-04 - mean_absolute_error: 0.0177 - val_loss: 3.3374e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 109/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0265e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 109: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 412ms/step - loss: 5.0265e-04 - mean_absolute_error: 0.0186 - val_loss: 3.9947e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 110/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0361e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 110: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 38s 442ms/step - loss: 5.0361e-04 - mean_absolute_error: 0.0186 - val_loss: 3.9199e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 111/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0564e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 111: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 37s 435ms/step - loss: 5.0564e-04 - mean_absolute_error: 0.0186 - val_loss: 3.3022e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 112/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7374e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 112: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 402ms/step - loss: 4.7374e-04 - mean_absolute_error: 0.0180 - val_loss: 3.5464e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 113/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9944e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 113: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.9944e-04 - mean_absolute_error: 0.0183 - val_loss: 3.5318e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 114/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2433e-04 - mean_absolute_error: 0.0192\n",
      "Epoch 114: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 411ms/step - loss: 5.2433e-04 - mean_absolute_error: 0.0192 - val_loss: 3.3958e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 115/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1709e-04 - mean_absolute_error: 0.0187\n",
      "Epoch 115: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 5.1709e-04 - mean_absolute_error: 0.0187 - val_loss: 4.9253e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 116/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.2789e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 116: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 377ms/step - loss: 5.2789e-04 - mean_absolute_error: 0.0190 - val_loss: 3.2505e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 117/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7076e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 117: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 382ms/step - loss: 4.7076e-04 - mean_absolute_error: 0.0178 - val_loss: 3.5360e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 118/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8965e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 118: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 399ms/step - loss: 4.8965e-04 - mean_absolute_error: 0.0179 - val_loss: 3.6109e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 119/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9377e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 119: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 380ms/step - loss: 4.9377e-04 - mean_absolute_error: 0.0186 - val_loss: 3.3365e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 120/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9300e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 120: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 375ms/step - loss: 4.9300e-04 - mean_absolute_error: 0.0182 - val_loss: 3.2308e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 121/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8721e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 121: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 387ms/step - loss: 4.8721e-04 - mean_absolute_error: 0.0181 - val_loss: 3.8702e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 122/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0449e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 122: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 5.0449e-04 - mean_absolute_error: 0.0183 - val_loss: 5.0340e-04 - val_mean_absolute_error: 0.0175\n",
      "Epoch 123/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7807e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 123: val_loss improved from 0.00032 to 0.00032, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 32s 371ms/step - loss: 4.7807e-04 - mean_absolute_error: 0.0181 - val_loss: 3.2012e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 124/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0594e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 124: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 377ms/step - loss: 5.0594e-04 - mean_absolute_error: 0.0188 - val_loss: 3.2442e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 125/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7561e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 125: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.7561e-04 - mean_absolute_error: 0.0178 - val_loss: 3.3285e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 126/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7392e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 126: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.7392e-04 - mean_absolute_error: 0.0178 - val_loss: 4.2264e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 127/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9235e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 127: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 376ms/step - loss: 4.9235e-04 - mean_absolute_error: 0.0181 - val_loss: 3.2348e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 128/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7839e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 128: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 378ms/step - loss: 4.7839e-04 - mean_absolute_error: 0.0179 - val_loss: 3.3130e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 129/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5662e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 129: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 402ms/step - loss: 4.5662e-04 - mean_absolute_error: 0.0177 - val_loss: 3.2830e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 130/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8602e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 130: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 394ms/step - loss: 4.8602e-04 - mean_absolute_error: 0.0179 - val_loss: 3.3335e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 131/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8883e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 131: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 408ms/step - loss: 4.8883e-04 - mean_absolute_error: 0.0182 - val_loss: 3.3417e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 132/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6797e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 132: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.6797e-04 - mean_absolute_error: 0.0178 - val_loss: 3.4525e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 133/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1117e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 133: val_loss improved from 0.00032 to 0.00032, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 5.1117e-04 - mean_absolute_error: 0.0189 - val_loss: 3.2011e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 134/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5388e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 134: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 385ms/step - loss: 4.5388e-04 - mean_absolute_error: 0.0176 - val_loss: 3.4346e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 135/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.0422e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 135: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 378ms/step - loss: 5.0422e-04 - mean_absolute_error: 0.0184 - val_loss: 3.3254e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 136/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7412e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 136: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 381ms/step - loss: 4.7412e-04 - mean_absolute_error: 0.0178 - val_loss: 3.3363e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 137/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7196e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 137: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 375ms/step - loss: 4.7196e-04 - mean_absolute_error: 0.0180 - val_loss: 3.4724e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 138/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8208e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 138: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 367ms/step - loss: 4.8208e-04 - mean_absolute_error: 0.0181 - val_loss: 3.8112e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 139/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8567e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 139: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 375ms/step - loss: 4.8567e-04 - mean_absolute_error: 0.0178 - val_loss: 3.2345e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 140/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7374e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 140: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.7374e-04 - mean_absolute_error: 0.0176 - val_loss: 3.2456e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 141/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5775e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 141: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 363ms/step - loss: 4.5775e-04 - mean_absolute_error: 0.0180 - val_loss: 3.6823e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 142/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6308e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 142: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 361ms/step - loss: 4.6308e-04 - mean_absolute_error: 0.0178 - val_loss: 3.2251e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 143/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9077e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 143: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 370ms/step - loss: 4.9077e-04 - mean_absolute_error: 0.0182 - val_loss: 3.3222e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 144/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8299e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 144: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 375ms/step - loss: 4.8299e-04 - mean_absolute_error: 0.0181 - val_loss: 3.3052e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 145/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.1033e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 145: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 360ms/step - loss: 5.1033e-04 - mean_absolute_error: 0.0183 - val_loss: 4.0234e-04 - val_mean_absolute_error: 0.0177\n",
      "Epoch 146/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9952e-04 - mean_absolute_error: 0.0186\n",
      "Epoch 146: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 359ms/step - loss: 4.9952e-04 - mean_absolute_error: 0.0186 - val_loss: 4.6510e-04 - val_mean_absolute_error: 0.0182\n",
      "Epoch 147/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8900e-04 - mean_absolute_error: 0.0184\n",
      "Epoch 147: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 372ms/step - loss: 4.8900e-04 - mean_absolute_error: 0.0184 - val_loss: 3.3508e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 148/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8795e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 148: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 382ms/step - loss: 4.8795e-04 - mean_absolute_error: 0.0182 - val_loss: 3.2418e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 149/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6362e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 149: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.6362e-04 - mean_absolute_error: 0.0173 - val_loss: 3.8887e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 150/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6416e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 150: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 383ms/step - loss: 4.6416e-04 - mean_absolute_error: 0.0177 - val_loss: 3.4790e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 151/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8270e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 151: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.8270e-04 - mean_absolute_error: 0.0178 - val_loss: 4.0827e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 152/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3511e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 152: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 382ms/step - loss: 4.3511e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2127e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 153/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9665e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 153: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 373ms/step - loss: 4.9665e-04 - mean_absolute_error: 0.0182 - val_loss: 3.2211e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 154/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6292e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 154: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 368ms/step - loss: 4.6292e-04 - mean_absolute_error: 0.0177 - val_loss: 3.2744e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 155/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6704e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 155: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 374ms/step - loss: 4.6704e-04 - mean_absolute_error: 0.0177 - val_loss: 3.2775e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 156/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7048e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 156: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 366ms/step - loss: 4.7048e-04 - mean_absolute_error: 0.0177 - val_loss: 3.2889e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 157/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7211e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 157: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 379ms/step - loss: 4.7211e-04 - mean_absolute_error: 0.0177 - val_loss: 3.7100e-04 - val_mean_absolute_error: 0.0167\n",
      "Epoch 158/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6770e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 158: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 372ms/step - loss: 4.6770e-04 - mean_absolute_error: 0.0178 - val_loss: 3.2313e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 159/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5732e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 159: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 380ms/step - loss: 4.5732e-04 - mean_absolute_error: 0.0175 - val_loss: 3.3044e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 160/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5454e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 160: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 367ms/step - loss: 4.5454e-04 - mean_absolute_error: 0.0174 - val_loss: 3.2382e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 161/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9156e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 161: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 364ms/step - loss: 4.9156e-04 - mean_absolute_error: 0.0182 - val_loss: 3.3527e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 162/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7325e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 162: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 371ms/step - loss: 4.7325e-04 - mean_absolute_error: 0.0178 - val_loss: 3.2439e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 163/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5602e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 163: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 380ms/step - loss: 4.5602e-04 - mean_absolute_error: 0.0175 - val_loss: 3.3180e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 164/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 5.3676e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 164: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 365ms/step - loss: 5.3676e-04 - mean_absolute_error: 0.0190 - val_loss: 3.4489e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 165/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5587e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 165: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 363ms/step - loss: 4.5587e-04 - mean_absolute_error: 0.0174 - val_loss: 3.3726e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 166/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8487e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 166: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 38s 452ms/step - loss: 4.8487e-04 - mean_absolute_error: 0.0178 - val_loss: 3.3337e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 167/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7192e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 167: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 387ms/step - loss: 4.7192e-04 - mean_absolute_error: 0.0180 - val_loss: 4.9831e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 168/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5934e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 168: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 370ms/step - loss: 4.5934e-04 - mean_absolute_error: 0.0175 - val_loss: 3.7384e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 169/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5968e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 169: val_loss improved from 0.00032 to 0.00032, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 32s 375ms/step - loss: 4.5968e-04 - mean_absolute_error: 0.0174 - val_loss: 3.1910e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 170/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7580e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 170: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 385ms/step - loss: 4.7580e-04 - mean_absolute_error: 0.0176 - val_loss: 3.9500e-04 - val_mean_absolute_error: 0.0145\n",
      "Epoch 171/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5901e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 171: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 389ms/step - loss: 4.5901e-04 - mean_absolute_error: 0.0176 - val_loss: 4.7397e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 172/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8056e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 172: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 402ms/step - loss: 4.8056e-04 - mean_absolute_error: 0.0179 - val_loss: 3.2627e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 173/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5930e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 173: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 409ms/step - loss: 4.5930e-04 - mean_absolute_error: 0.0174 - val_loss: 4.2928e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 174/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6299e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 174: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 414ms/step - loss: 4.6299e-04 - mean_absolute_error: 0.0178 - val_loss: 3.3405e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 175/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7847e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 175: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.7847e-04 - mean_absolute_error: 0.0180 - val_loss: 3.2045e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 176/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7977e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 176: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.7977e-04 - mean_absolute_error: 0.0177 - val_loss: 3.2767e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 177/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8645e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 177: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 394ms/step - loss: 4.8645e-04 - mean_absolute_error: 0.0180 - val_loss: 3.6705e-04 - val_mean_absolute_error: 0.0163\n",
      "Epoch 178/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6483e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 178: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.6483e-04 - mean_absolute_error: 0.0180 - val_loss: 3.5353e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 179/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5759e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 179: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 377ms/step - loss: 4.5759e-04 - mean_absolute_error: 0.0174 - val_loss: 3.2916e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 180/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5093e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 180: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 383ms/step - loss: 4.5093e-04 - mean_absolute_error: 0.0171 - val_loss: 4.2633e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 181/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5219e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 181: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 412ms/step - loss: 4.5219e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2264e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 182/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6236e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 182: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.6236e-04 - mean_absolute_error: 0.0175 - val_loss: 3.3471e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 183/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5886e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 183: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.5886e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2970e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 184/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6757e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 184: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 38s 446ms/step - loss: 4.6757e-04 - mean_absolute_error: 0.0176 - val_loss: 3.4110e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 185/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5035e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 185: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.5035e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3650e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 186/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6005e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 186: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 376ms/step - loss: 4.6005e-04 - mean_absolute_error: 0.0177 - val_loss: 4.3546e-04 - val_mean_absolute_error: 0.0166\n",
      "Epoch 187/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7354e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 187: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.7354e-04 - mean_absolute_error: 0.0178 - val_loss: 3.4048e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 188/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5968e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 188: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.5968e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2940e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 189/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3774e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 189: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 381ms/step - loss: 4.3774e-04 - mean_absolute_error: 0.0169 - val_loss: 3.6005e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 190/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4954e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 190: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 409ms/step - loss: 4.4954e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3083e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 191/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6046e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 191: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.6046e-04 - mean_absolute_error: 0.0176 - val_loss: 3.3999e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 192/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6334e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 192: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.6334e-04 - mean_absolute_error: 0.0177 - val_loss: 4.0937e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 193/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6268e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 193: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 377ms/step - loss: 4.6268e-04 - mean_absolute_error: 0.0175 - val_loss: 3.3162e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 194/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5960e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 194: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 379ms/step - loss: 4.5960e-04 - mean_absolute_error: 0.0179 - val_loss: 3.5696e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 195/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4982e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 195: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.4982e-04 - mean_absolute_error: 0.0174 - val_loss: 3.7684e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 196/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8755e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 196: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 4.8755e-04 - mean_absolute_error: 0.0182 - val_loss: 3.4344e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 197/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3412e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 197: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 374ms/step - loss: 4.3412e-04 - mean_absolute_error: 0.0169 - val_loss: 3.3319e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 198/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7353e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 198: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 383ms/step - loss: 4.7353e-04 - mean_absolute_error: 0.0178 - val_loss: 3.5654e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 199/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4496e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 199: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 392ms/step - loss: 4.4496e-04 - mean_absolute_error: 0.0172 - val_loss: 3.3042e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 200/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3538e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 200: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 377ms/step - loss: 4.3538e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8508e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 201/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4393e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 201: val_loss improved from 0.00032 to 0.00032, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 32s 374ms/step - loss: 4.4393e-04 - mean_absolute_error: 0.0172 - val_loss: 3.1784e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 202/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4054e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 202: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.4054e-04 - mean_absolute_error: 0.0173 - val_loss: 4.0714e-04 - val_mean_absolute_error: 0.0173\n",
      "Epoch 203/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4042e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 203: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 401ms/step - loss: 4.4042e-04 - mean_absolute_error: 0.0170 - val_loss: 3.3924e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 204/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9614e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 204: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 372ms/step - loss: 4.9614e-04 - mean_absolute_error: 0.0179 - val_loss: 4.1845e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 205/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5479e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 205: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 367ms/step - loss: 4.5479e-04 - mean_absolute_error: 0.0176 - val_loss: 3.3555e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 206/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6202e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 206: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 392ms/step - loss: 4.6202e-04 - mean_absolute_error: 0.0176 - val_loss: 3.2438e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 207/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4661e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 207: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 376ms/step - loss: 4.4661e-04 - mean_absolute_error: 0.0172 - val_loss: 3.2688e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 208/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7845e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 208: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 370ms/step - loss: 4.7845e-04 - mean_absolute_error: 0.0175 - val_loss: 3.3107e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 209/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5208e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 209: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 372ms/step - loss: 4.5208e-04 - mean_absolute_error: 0.0172 - val_loss: 3.3635e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 210/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7647e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 210: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 392ms/step - loss: 4.7647e-04 - mean_absolute_error: 0.0177 - val_loss: 3.3422e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 211/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6241e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 211: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 4.6241e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2649e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 212/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5682e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 212: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.5682e-04 - mean_absolute_error: 0.0176 - val_loss: 3.3036e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 213/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6824e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 213: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.6824e-04 - mean_absolute_error: 0.0176 - val_loss: 3.4107e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 214/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6047e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 214: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 396ms/step - loss: 4.6047e-04 - mean_absolute_error: 0.0175 - val_loss: 3.4030e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 215/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7915e-04 - mean_absolute_error: 0.0180\n",
      "Epoch 215: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 383ms/step - loss: 4.7915e-04 - mean_absolute_error: 0.0180 - val_loss: 3.8749e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 216/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4923e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 216: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 372ms/step - loss: 4.4923e-04 - mean_absolute_error: 0.0173 - val_loss: 3.9927e-04 - val_mean_absolute_error: 0.0151\n",
      "Epoch 217/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6191e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 217: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 385ms/step - loss: 4.6191e-04 - mean_absolute_error: 0.0173 - val_loss: 3.2299e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 218/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6105e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 218: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 377ms/step - loss: 4.6105e-04 - mean_absolute_error: 0.0174 - val_loss: 3.3354e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 219/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7399e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 219: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 370ms/step - loss: 4.7399e-04 - mean_absolute_error: 0.0177 - val_loss: 3.5748e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 220/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5351e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 220: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 370ms/step - loss: 4.5351e-04 - mean_absolute_error: 0.0171 - val_loss: 3.9589e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 221/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8022e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 221: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 381ms/step - loss: 4.8022e-04 - mean_absolute_error: 0.0177 - val_loss: 3.2432e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 222/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6393e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 222: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 369ms/step - loss: 4.6393e-04 - mean_absolute_error: 0.0174 - val_loss: 3.2269e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 223/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4992e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 223: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 366ms/step - loss: 4.4992e-04 - mean_absolute_error: 0.0172 - val_loss: 3.3886e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 224/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3988e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 224: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 375ms/step - loss: 4.3988e-04 - mean_absolute_error: 0.0170 - val_loss: 4.6219e-04 - val_mean_absolute_error: 0.0171\n",
      "Epoch 225/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9007e-04 - mean_absolute_error: 0.0182\n",
      "Epoch 225: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 4.9007e-04 - mean_absolute_error: 0.0182 - val_loss: 3.6833e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 226/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6086e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 226: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 369ms/step - loss: 4.6086e-04 - mean_absolute_error: 0.0176 - val_loss: 3.2628e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 227/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6845e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 227: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 366ms/step - loss: 4.6845e-04 - mean_absolute_error: 0.0174 - val_loss: 3.7956e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 228/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8661e-04 - mean_absolute_error: 0.0181\n",
      "Epoch 228: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 381ms/step - loss: 4.8661e-04 - mean_absolute_error: 0.0181 - val_loss: 3.2572e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 229/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.8102e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 229: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 385ms/step - loss: 4.8102e-04 - mean_absolute_error: 0.0178 - val_loss: 3.8258e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 230/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6252e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 230: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 371ms/step - loss: 4.6252e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2700e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 231/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4035e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 231: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 379ms/step - loss: 4.4035e-04 - mean_absolute_error: 0.0173 - val_loss: 4.5510e-04 - val_mean_absolute_error: 0.0160\n",
      "Epoch 232/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6038e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 232: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 381ms/step - loss: 4.6038e-04 - mean_absolute_error: 0.0174 - val_loss: 3.2017e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 233/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5354e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 233: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 374ms/step - loss: 4.5354e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2484e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 234/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4747e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 234: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 365ms/step - loss: 4.4747e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2593e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 235/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5860e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 235: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 368ms/step - loss: 4.5860e-04 - mean_absolute_error: 0.0175 - val_loss: 3.1949e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 236/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7809e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 236: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 382ms/step - loss: 4.7809e-04 - mean_absolute_error: 0.0177 - val_loss: 3.2255e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 237/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4806e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 237: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 408ms/step - loss: 4.4806e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2859e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 238/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6070e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 238: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 375ms/step - loss: 4.6070e-04 - mean_absolute_error: 0.0173 - val_loss: 4.0563e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 239/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4308e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 239: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 375ms/step - loss: 4.4308e-04 - mean_absolute_error: 0.0172 - val_loss: 3.6060e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 240/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5665e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 240: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 4.5665e-04 - mean_absolute_error: 0.0177 - val_loss: 3.2430e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 241/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2633e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 241: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 415ms/step - loss: 4.2633e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2570e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 242/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5876e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 242: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 390ms/step - loss: 4.5876e-04 - mean_absolute_error: 0.0176 - val_loss: 3.3961e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 243/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3668e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 243: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 394ms/step - loss: 4.3668e-04 - mean_absolute_error: 0.0170 - val_loss: 3.4135e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 244/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5328e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 244: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.5328e-04 - mean_absolute_error: 0.0175 - val_loss: 3.3216e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 245/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5736e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 245: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 383ms/step - loss: 4.5736e-04 - mean_absolute_error: 0.0174 - val_loss: 3.4742e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 246/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3034e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 246: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 382ms/step - loss: 4.3034e-04 - mean_absolute_error: 0.0171 - val_loss: 4.0327e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 247/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2066e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 247: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 394ms/step - loss: 4.2066e-04 - mean_absolute_error: 0.0167 - val_loss: 3.3713e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 248/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3263e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 248: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.3263e-04 - mean_absolute_error: 0.0172 - val_loss: 3.4444e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 249/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7394e-04 - mean_absolute_error: 0.0178\n",
      "Epoch 249: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 375ms/step - loss: 4.7394e-04 - mean_absolute_error: 0.0178 - val_loss: 3.2504e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 250/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3143e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 250: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.3143e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2692e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 251/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6386e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 251: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.6386e-04 - mean_absolute_error: 0.0174 - val_loss: 3.2185e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 252/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7574e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 252: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 374ms/step - loss: 4.7574e-04 - mean_absolute_error: 0.0176 - val_loss: 3.8448e-04 - val_mean_absolute_error: 0.0156\n",
      "Epoch 253/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5064e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 253: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 366ms/step - loss: 4.5064e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3371e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 254/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7828e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 254: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.7828e-04 - mean_absolute_error: 0.0179 - val_loss: 3.3536e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 255/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3287e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 255: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.3287e-04 - mean_absolute_error: 0.0170 - val_loss: 3.6274e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 256/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4373e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 256: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 378ms/step - loss: 4.4373e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2527e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 257/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6575e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 257: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 365ms/step - loss: 4.6575e-04 - mean_absolute_error: 0.0174 - val_loss: 4.1374e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 258/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4584e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 258: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 4.4584e-04 - mean_absolute_error: 0.0171 - val_loss: 3.4328e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 259/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5124e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 259: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 380ms/step - loss: 4.5124e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3950e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 260/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2409e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 260: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 367ms/step - loss: 4.2409e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7014e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 261/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7189e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 261: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 376ms/step - loss: 4.7189e-04 - mean_absolute_error: 0.0173 - val_loss: 3.4197e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 262/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4868e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 262: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 380ms/step - loss: 4.4868e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3322e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 263/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5961e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 263: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 373ms/step - loss: 4.5961e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2183e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 264/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3415e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 264: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 363ms/step - loss: 4.3415e-04 - mean_absolute_error: 0.0170 - val_loss: 3.3710e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 265/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.9883e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 265: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 374ms/step - loss: 4.9883e-04 - mean_absolute_error: 0.0179 - val_loss: 3.4647e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 266/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3567e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 266: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 381ms/step - loss: 4.3567e-04 - mean_absolute_error: 0.0173 - val_loss: 3.2135e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 267/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3785e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 267: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 365ms/step - loss: 4.3785e-04 - mean_absolute_error: 0.0171 - val_loss: 3.3112e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 268/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2214e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 268: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 362ms/step - loss: 4.2214e-04 - mean_absolute_error: 0.0168 - val_loss: 3.5664e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 269/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6989e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 269: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 377ms/step - loss: 4.6989e-04 - mean_absolute_error: 0.0177 - val_loss: 3.7903e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 270/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4920e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 270: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 378ms/step - loss: 4.4920e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2782e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 271/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4633e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 271: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 367ms/step - loss: 4.4633e-04 - mean_absolute_error: 0.0174 - val_loss: 3.4633e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 272/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4322e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 272: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 363ms/step - loss: 4.4322e-04 - mean_absolute_error: 0.0170 - val_loss: 3.3737e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 273/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1733e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 273: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.1733e-04 - mean_absolute_error: 0.0167 - val_loss: 3.4936e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 274/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3978e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 274: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 390ms/step - loss: 4.3978e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2771e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 275/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4316e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 275: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 379ms/step - loss: 4.4316e-04 - mean_absolute_error: 0.0171 - val_loss: 3.6358e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 276/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5218e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 276: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.5218e-04 - mean_absolute_error: 0.0171 - val_loss: 3.6600e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 277/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4900e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 277: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.4900e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2047e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 278/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5874e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 278: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.5874e-04 - mean_absolute_error: 0.0174 - val_loss: 3.3002e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 279/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5465e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 279: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 367ms/step - loss: 4.5465e-04 - mean_absolute_error: 0.0174 - val_loss: 3.2452e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 280/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1450e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 280: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 374ms/step - loss: 4.1450e-04 - mean_absolute_error: 0.0166 - val_loss: 3.3965e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 281/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4995e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 281: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 374ms/step - loss: 4.4995e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2582e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 282/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3322e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 282: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 30s 358ms/step - loss: 4.3322e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2353e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 283/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4047e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 283: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 364ms/step - loss: 4.4047e-04 - mean_absolute_error: 0.0171 - val_loss: 3.3070e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 284/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5942e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 284: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 376ms/step - loss: 4.5942e-04 - mean_absolute_error: 0.0172 - val_loss: 3.9588e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 285/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7378e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 285: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 377ms/step - loss: 4.7378e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2113e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 286/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3618e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 286: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 30s 358ms/step - loss: 4.3618e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2046e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 287/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3947e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 287: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 30s 355ms/step - loss: 4.3947e-04 - mean_absolute_error: 0.0171 - val_loss: 3.5982e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 288/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3266e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 288: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 373ms/step - loss: 4.3266e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2456e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 289/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5544e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 289: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 373ms/step - loss: 4.5544e-04 - mean_absolute_error: 0.0175 - val_loss: 3.6120e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 290/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5243e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 290: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 30s 358ms/step - loss: 4.5243e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2091e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 291/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6058e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 291: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 30s 359ms/step - loss: 4.6058e-04 - mean_absolute_error: 0.0172 - val_loss: 3.3532e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 292/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5637e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 292: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 402ms/step - loss: 4.5637e-04 - mean_absolute_error: 0.0173 - val_loss: 3.2686e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 293/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6823e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 293: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 376ms/step - loss: 4.6823e-04 - mean_absolute_error: 0.0179 - val_loss: 3.8557e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 294/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7055e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 294: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 360ms/step - loss: 4.7055e-04 - mean_absolute_error: 0.0177 - val_loss: 3.2059e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 295/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3204e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 295: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 360ms/step - loss: 4.3204e-04 - mean_absolute_error: 0.0169 - val_loss: 3.1862e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 296/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1609e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 296: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 376ms/step - loss: 4.1609e-04 - mean_absolute_error: 0.0166 - val_loss: 3.1972e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 297/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.7943e-04 - mean_absolute_error: 0.0183\n",
      "Epoch 297: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 366ms/step - loss: 4.7943e-04 - mean_absolute_error: 0.0183 - val_loss: 3.8678e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 298/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5625e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 298: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 373ms/step - loss: 4.5625e-04 - mean_absolute_error: 0.0174 - val_loss: 3.4922e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 299/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5163e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 299: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.5163e-04 - mean_absolute_error: 0.0173 - val_loss: 3.1844e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 300/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4059e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 300: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.4059e-04 - mean_absolute_error: 0.0170 - val_loss: 3.5671e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 301/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2900e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 301: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 385ms/step - loss: 4.2900e-04 - mean_absolute_error: 0.0170 - val_loss: 3.4984e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 302/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5205e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 302: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 365ms/step - loss: 4.5205e-04 - mean_absolute_error: 0.0172 - val_loss: 3.1908e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 303/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4056e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 303: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 371ms/step - loss: 4.4056e-04 - mean_absolute_error: 0.0167 - val_loss: 4.6281e-04 - val_mean_absolute_error: 0.0154\n",
      "Epoch 304/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6140e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 304: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 376ms/step - loss: 4.6140e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2262e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 305/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3735e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 305: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 4.3735e-04 - mean_absolute_error: 0.0170 - val_loss: 3.1881e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 306/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4296e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 306: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 379ms/step - loss: 4.4296e-04 - mean_absolute_error: 0.0170 - val_loss: 3.1893e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 307/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5559e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 307: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 402ms/step - loss: 4.5559e-04 - mean_absolute_error: 0.0172 - val_loss: 3.3047e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 308/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5605e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 308: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 408ms/step - loss: 4.5605e-04 - mean_absolute_error: 0.0170 - val_loss: 3.5896e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 309/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5703e-04 - mean_absolute_error: 0.0177\n",
      "Epoch 309: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 412ms/step - loss: 4.5703e-04 - mean_absolute_error: 0.0177 - val_loss: 3.4569e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 310/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5124e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 310: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 411ms/step - loss: 4.5124e-04 - mean_absolute_error: 0.0175 - val_loss: 3.4118e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 311/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4843e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 311: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 410ms/step - loss: 4.4843e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3263e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 312/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6708e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 312: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.6708e-04 - mean_absolute_error: 0.0175 - val_loss: 3.9250e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 313/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4624e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 313: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.4624e-04 - mean_absolute_error: 0.0170 - val_loss: 3.7454e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 314/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4597e-04 - mean_absolute_error: 0.0176\n",
      "Epoch 314: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.4597e-04 - mean_absolute_error: 0.0176 - val_loss: 3.9249e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 315/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6658e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 315: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 415ms/step - loss: 4.6658e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2619e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 316/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3159e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 316: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 398ms/step - loss: 4.3159e-04 - mean_absolute_error: 0.0171 - val_loss: 3.1785e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 317/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4364e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 317: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.4364e-04 - mean_absolute_error: 0.0171 - val_loss: 3.1829e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 318/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4247e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 318: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.4247e-04 - mean_absolute_error: 0.0172 - val_loss: 3.2112e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 319/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5967e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 319: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.5967e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2461e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 320/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3336e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 320: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 373ms/step - loss: 4.3336e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2336e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 321/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4783e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 321: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 380ms/step - loss: 4.4783e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2734e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 322/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2479e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 322: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 381ms/step - loss: 4.2479e-04 - mean_absolute_error: 0.0167 - val_loss: 3.6968e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 323/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4226e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 323: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 31s 368ms/step - loss: 4.4226e-04 - mean_absolute_error: 0.0172 - val_loss: 3.1805e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 324/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3890e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 324: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 380ms/step - loss: 4.3890e-04 - mean_absolute_error: 0.0173 - val_loss: 3.2241e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 325/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4198e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 325: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.4198e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2390e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 326/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3985e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 326: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.3985e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2806e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 327/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3277e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 327: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 32s 382ms/step - loss: 4.3277e-04 - mean_absolute_error: 0.0168 - val_loss: 3.4947e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 328/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4752e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 328: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 405ms/step - loss: 4.4752e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2295e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 329/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6655e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 329: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 36s 419ms/step - loss: 4.6655e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2440e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 330/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6077e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 330: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.6077e-04 - mean_absolute_error: 0.0171 - val_loss: 3.5652e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 331/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3128e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 331: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 398ms/step - loss: 4.3128e-04 - mean_absolute_error: 0.0170 - val_loss: 3.4687e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 332/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4738e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 332: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 415ms/step - loss: 4.4738e-04 - mean_absolute_error: 0.0171 - val_loss: 3.3276e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 333/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3470e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 333: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 405ms/step - loss: 4.3470e-04 - mean_absolute_error: 0.0169 - val_loss: 3.3987e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 334/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2244e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 334: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.2244e-04 - mean_absolute_error: 0.0168 - val_loss: 3.7234e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 335/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4099e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 335: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 404ms/step - loss: 4.4099e-04 - mean_absolute_error: 0.0171 - val_loss: 3.3148e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 336/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3176e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 336: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 36s 420ms/step - loss: 4.3176e-04 - mean_absolute_error: 0.0169 - val_loss: 3.3766e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 337/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2957e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 337: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 408ms/step - loss: 4.2957e-04 - mean_absolute_error: 0.0169 - val_loss: 3.2231e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 338/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3226e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 338: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 399ms/step - loss: 4.3226e-04 - mean_absolute_error: 0.0169 - val_loss: 3.3462e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 339/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6012e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 339: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 416ms/step - loss: 4.6012e-04 - mean_absolute_error: 0.0174 - val_loss: 3.3519e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 340/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6245e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 340: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 38s 444ms/step - loss: 4.6245e-04 - mean_absolute_error: 0.0174 - val_loss: 3.3641e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 341/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3968e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 341: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 398ms/step - loss: 4.3968e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2505e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 342/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3569e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 342: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 401ms/step - loss: 4.3569e-04 - mean_absolute_error: 0.0169 - val_loss: 3.2500e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 343/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1858e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 343: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 401ms/step - loss: 4.1858e-04 - mean_absolute_error: 0.0168 - val_loss: 3.6806e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 344/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2556e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 344: val_loss improved from 0.00032 to 0.00032, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 34s 398ms/step - loss: 4.2556e-04 - mean_absolute_error: 0.0168 - val_loss: 3.1717e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 345/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5317e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 345: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.5317e-04 - mean_absolute_error: 0.0170 - val_loss: 3.3733e-04 - val_mean_absolute_error: 0.0144\n",
      "Epoch 346/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5508e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 346: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.5508e-04 - mean_absolute_error: 0.0173 - val_loss: 3.2687e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 347/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5486e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 347: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.5486e-04 - mean_absolute_error: 0.0174 - val_loss: 3.2147e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 348/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5292e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 348: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 387ms/step - loss: 4.5292e-04 - mean_absolute_error: 0.0170 - val_loss: 3.4977e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 349/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4441e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 349: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.4441e-04 - mean_absolute_error: 0.0172 - val_loss: 3.2065e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 350/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5306e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 350: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 4.5306e-04 - mean_absolute_error: 0.0172 - val_loss: 3.3082e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 351/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1618e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 351: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 390ms/step - loss: 4.1618e-04 - mean_absolute_error: 0.0167 - val_loss: 3.4492e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 352/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2967e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 352: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.2967e-04 - mean_absolute_error: 0.0167 - val_loss: 3.3069e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 353/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5043e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 353: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 404ms/step - loss: 4.5043e-04 - mean_absolute_error: 0.0172 - val_loss: 3.2092e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 354/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2427e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 354: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 404ms/step - loss: 4.2427e-04 - mean_absolute_error: 0.0168 - val_loss: 3.2017e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 355/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2513e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 355: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 390ms/step - loss: 4.2513e-04 - mean_absolute_error: 0.0167 - val_loss: 3.3473e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 356/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2872e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 356: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 390ms/step - loss: 4.2872e-04 - mean_absolute_error: 0.0169 - val_loss: 3.2004e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 357/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4277e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 357: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 402ms/step - loss: 4.4277e-04 - mean_absolute_error: 0.0169 - val_loss: 3.4056e-04 - val_mean_absolute_error: 0.0138\n",
      "Epoch 358/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3665e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 358: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 36s 421ms/step - loss: 4.3665e-04 - mean_absolute_error: 0.0170 - val_loss: 3.3389e-04 - val_mean_absolute_error: 0.0149\n",
      "Epoch 359/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5319e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 359: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.5319e-04 - mean_absolute_error: 0.0172 - val_loss: 3.2198e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 360/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1841e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 360: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 397ms/step - loss: 4.1841e-04 - mean_absolute_error: 0.0169 - val_loss: 3.4466e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 361/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4718e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 361: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.4718e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2155e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 362/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4208e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 362: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 385ms/step - loss: 4.4208e-04 - mean_absolute_error: 0.0170 - val_loss: 3.1792e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 363/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5418e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 363: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.5418e-04 - mean_absolute_error: 0.0172 - val_loss: 3.2520e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 364/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5811e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 364: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 4.5811e-04 - mean_absolute_error: 0.0175 - val_loss: 3.3991e-04 - val_mean_absolute_error: 0.0157\n",
      "Epoch 365/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5045e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 365: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 404ms/step - loss: 4.5045e-04 - mean_absolute_error: 0.0174 - val_loss: 3.7979e-04 - val_mean_absolute_error: 0.0133\n",
      "Epoch 366/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2332e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 366: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.2332e-04 - mean_absolute_error: 0.0169 - val_loss: 3.3192e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 367/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3249e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 367: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 411ms/step - loss: 4.3249e-04 - mean_absolute_error: 0.0170 - val_loss: 3.5080e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 368/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4181e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 368: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 416ms/step - loss: 4.4181e-04 - mean_absolute_error: 0.0171 - val_loss: 3.5641e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 369/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2549e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 369: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 402ms/step - loss: 4.2549e-04 - mean_absolute_error: 0.0167 - val_loss: 3.3502e-04 - val_mean_absolute_error: 0.0142\n",
      "Epoch 370/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2369e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 370: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 401ms/step - loss: 4.2369e-04 - mean_absolute_error: 0.0165 - val_loss: 3.2078e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 371/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4598e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 371: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 417ms/step - loss: 4.4598e-04 - mean_absolute_error: 0.0171 - val_loss: 3.1841e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 372/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2434e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 372: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 411ms/step - loss: 4.2434e-04 - mean_absolute_error: 0.0167 - val_loss: 3.3063e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 373/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4794e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 373: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 404ms/step - loss: 4.4794e-04 - mean_absolute_error: 0.0173 - val_loss: 3.1906e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 374/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3512e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 374: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 417ms/step - loss: 4.3512e-04 - mean_absolute_error: 0.0168 - val_loss: 4.0436e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 375/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5587e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 375: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 398ms/step - loss: 4.5587e-04 - mean_absolute_error: 0.0173 - val_loss: 3.2757e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 376/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4117e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 376: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.4117e-04 - mean_absolute_error: 0.0169 - val_loss: 3.2185e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 377/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3891e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 377: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 394ms/step - loss: 4.3891e-04 - mean_absolute_error: 0.0168 - val_loss: 3.3769e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 378/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5737e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 378: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 408ms/step - loss: 4.5737e-04 - mean_absolute_error: 0.0172 - val_loss: 3.3460e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 379/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0978e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 379: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 396ms/step - loss: 4.0978e-04 - mean_absolute_error: 0.0166 - val_loss: 3.2508e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 380/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2918e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 380: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.2918e-04 - mean_absolute_error: 0.0168 - val_loss: 3.2910e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 381/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2524e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 381: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 402ms/step - loss: 4.2524e-04 - mean_absolute_error: 0.0168 - val_loss: 3.3192e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 382/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4658e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 382: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 412ms/step - loss: 4.4658e-04 - mean_absolute_error: 0.0171 - val_loss: 3.3938e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 383/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4376e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 383: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.4376e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2665e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 384/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6040e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 384: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 394ms/step - loss: 4.6040e-04 - mean_absolute_error: 0.0175 - val_loss: 3.2978e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 385/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4081e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 385: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 412ms/step - loss: 4.4081e-04 - mean_absolute_error: 0.0172 - val_loss: 3.2420e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 386/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4872e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 386: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 406ms/step - loss: 4.4872e-04 - mean_absolute_error: 0.0170 - val_loss: 3.3707e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 387/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1397e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 387: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.1397e-04 - mean_absolute_error: 0.0166 - val_loss: 3.2484e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 388/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5777e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 388: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 37s 430ms/step - loss: 4.5777e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3206e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 389/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1996e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 389: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 412ms/step - loss: 4.1996e-04 - mean_absolute_error: 0.0167 - val_loss: 3.4053e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 390/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0894e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 390: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 389ms/step - loss: 4.0894e-04 - mean_absolute_error: 0.0163 - val_loss: 3.2356e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 391/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3617e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 391: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.3617e-04 - mean_absolute_error: 0.0168 - val_loss: 3.3257e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 392/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4375e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 392: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 412ms/step - loss: 4.4375e-04 - mean_absolute_error: 0.0171 - val_loss: 3.1883e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 393/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3094e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 393: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 406ms/step - loss: 4.3094e-04 - mean_absolute_error: 0.0170 - val_loss: 3.1918e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 394/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3984e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 394: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.3984e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2328e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 395/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0904e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 395: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 408ms/step - loss: 4.0904e-04 - mean_absolute_error: 0.0167 - val_loss: 3.4848e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 396/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3848e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 396: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 36s 427ms/step - loss: 4.3848e-04 - mean_absolute_error: 0.0168 - val_loss: 3.3517e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 397/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3226e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 397: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.3226e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2158e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 398/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4134e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 398: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 410ms/step - loss: 4.4134e-04 - mean_absolute_error: 0.0170 - val_loss: 3.6572e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 399/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5818e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 399: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 36s 422ms/step - loss: 4.5818e-04 - mean_absolute_error: 0.0173 - val_loss: 3.8975e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 400/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2822e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 400: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 35s 418ms/step - loss: 4.2822e-04 - mean_absolute_error: 0.0171 - val_loss: 3.4112e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 401/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4567e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 401: val_loss improved from 0.00032 to 0.00032, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 35s 413ms/step - loss: 4.4567e-04 - mean_absolute_error: 0.0171 - val_loss: 3.1559e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 402/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5210e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 402: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.5210e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3897e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 403/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1260e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 403: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.1260e-04 - mean_absolute_error: 0.0167 - val_loss: 3.2078e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 404/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4581e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 404: val_loss improved from 0.00032 to 0.00032, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 33s 384ms/step - loss: 4.4581e-04 - mean_absolute_error: 0.0171 - val_loss: 3.1523e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 405/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2537e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 405: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 386ms/step - loss: 4.2537e-04 - mean_absolute_error: 0.0166 - val_loss: 3.1735e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 406/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3350e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 406: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 402ms/step - loss: 4.3350e-04 - mean_absolute_error: 0.0167 - val_loss: 3.2217e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 407/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5062e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 407: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.5062e-04 - mean_absolute_error: 0.0172 - val_loss: 3.4280e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 408/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4577e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 408: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.4577e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3149e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 409/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2374e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 409: val_loss did not improve from 0.00032\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 4.2374e-04 - mean_absolute_error: 0.0167 - val_loss: 3.5233e-04 - val_mean_absolute_error: 0.0126\n",
      "Epoch 410/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3462e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 410: val_loss improved from 0.00032 to 0.00031, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 35s 415ms/step - loss: 4.3462e-04 - mean_absolute_error: 0.0169 - val_loss: 3.1423e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 411/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1597e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 411: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.1597e-04 - mean_absolute_error: 0.0165 - val_loss: 3.2119e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 412/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3007e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 412: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 389ms/step - loss: 4.3007e-04 - mean_absolute_error: 0.0169 - val_loss: 4.3518e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 413/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3364e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 413: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 406ms/step - loss: 4.3364e-04 - mean_absolute_error: 0.0168 - val_loss: 3.1739e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 414/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4234e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 414: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.4234e-04 - mean_absolute_error: 0.0170 - val_loss: 3.2622e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 415/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3362e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 415: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 404ms/step - loss: 4.3362e-04 - mean_absolute_error: 0.0168 - val_loss: 3.3789e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 416/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2865e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 416: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.2865e-04 - mean_absolute_error: 0.0169 - val_loss: 3.2206e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 417/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.6910e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 417: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 411ms/step - loss: 4.6910e-04 - mean_absolute_error: 0.0174 - val_loss: 3.5279e-04 - val_mean_absolute_error: 0.0147\n",
      "Epoch 418/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3456e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 418: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 392ms/step - loss: 4.3456e-04 - mean_absolute_error: 0.0171 - val_loss: 3.1515e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 419/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1440e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 419: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 398ms/step - loss: 4.1440e-04 - mean_absolute_error: 0.0168 - val_loss: 3.4224e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 420/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1994e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 420: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.1994e-04 - mean_absolute_error: 0.0165 - val_loss: 3.1776e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 421/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3923e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 421: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.3923e-04 - mean_absolute_error: 0.0171 - val_loss: 3.1638e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 422/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2179e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 422: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.2179e-04 - mean_absolute_error: 0.0167 - val_loss: 3.2425e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 423/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2314e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 423: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.2314e-04 - mean_absolute_error: 0.0169 - val_loss: 3.3189e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 424/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0361e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 424: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 412ms/step - loss: 4.0361e-04 - mean_absolute_error: 0.0163 - val_loss: 3.2971e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 425/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4024e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 425: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.4024e-04 - mean_absolute_error: 0.0168 - val_loss: 3.6462e-04 - val_mean_absolute_error: 0.0146\n",
      "Epoch 426/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2683e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 426: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 390ms/step - loss: 4.2683e-04 - mean_absolute_error: 0.0168 - val_loss: 3.1943e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 427/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1362e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 427: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 416ms/step - loss: 4.1362e-04 - mean_absolute_error: 0.0165 - val_loss: 3.4270e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 428/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2865e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 428: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 404ms/step - loss: 4.2865e-04 - mean_absolute_error: 0.0169 - val_loss: 3.1549e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 429/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1987e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 429: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 390ms/step - loss: 4.1987e-04 - mean_absolute_error: 0.0166 - val_loss: 3.6492e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 430/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2917e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 430: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 399ms/step - loss: 4.2917e-04 - mean_absolute_error: 0.0166 - val_loss: 3.2866e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 431/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2498e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 431: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 409ms/step - loss: 4.2498e-04 - mean_absolute_error: 0.0168 - val_loss: 3.4910e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 432/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2800e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 432: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 394ms/step - loss: 4.2800e-04 - mean_absolute_error: 0.0168 - val_loss: 3.2293e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 433/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4038e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 433: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 388ms/step - loss: 4.4038e-04 - mean_absolute_error: 0.0169 - val_loss: 3.1589e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 434/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3579e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 434: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.3579e-04 - mean_absolute_error: 0.0168 - val_loss: 3.6529e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 435/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3843e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 435: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 405ms/step - loss: 4.3843e-04 - mean_absolute_error: 0.0167 - val_loss: 3.5321e-04 - val_mean_absolute_error: 0.0134\n",
      "Epoch 436/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3898e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 436: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 405ms/step - loss: 4.3898e-04 - mean_absolute_error: 0.0169 - val_loss: 3.2372e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 437/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1878e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 437: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.1878e-04 - mean_absolute_error: 0.0167 - val_loss: 3.1550e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 438/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5417e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 438: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 411ms/step - loss: 4.5417e-04 - mean_absolute_error: 0.0171 - val_loss: 4.0669e-04 - val_mean_absolute_error: 0.0137\n",
      "Epoch 439/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4087e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 439: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.4087e-04 - mean_absolute_error: 0.0169 - val_loss: 3.3743e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 440/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2961e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 440: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 387ms/step - loss: 4.2961e-04 - mean_absolute_error: 0.0169 - val_loss: 3.1845e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 441/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4393e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 441: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 4.4393e-04 - mean_absolute_error: 0.0170 - val_loss: 3.1434e-04 - val_mean_absolute_error: 0.0114\n",
      "Epoch 442/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2114e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 442: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.2114e-04 - mean_absolute_error: 0.0167 - val_loss: 3.6532e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 443/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2880e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 443: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 387ms/step - loss: 4.2880e-04 - mean_absolute_error: 0.0169 - val_loss: 3.1626e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 444/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5466e-04 - mean_absolute_error: 0.0174\n",
      "Epoch 444: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.5466e-04 - mean_absolute_error: 0.0174 - val_loss: 4.1574e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 445/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2238e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 445: val_loss improved from 0.00031 to 0.00031, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 35s 410ms/step - loss: 4.2238e-04 - mean_absolute_error: 0.0167 - val_loss: 3.1395e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 446/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3413e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 446: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 398ms/step - loss: 4.3413e-04 - mean_absolute_error: 0.0169 - val_loss: 3.5804e-04 - val_mean_absolute_error: 0.0141\n",
      "Epoch 447/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4194e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 447: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 389ms/step - loss: 4.4194e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7102e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 448/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1654e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 448: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 406ms/step - loss: 4.1654e-04 - mean_absolute_error: 0.0166 - val_loss: 3.2617e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 449/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1555e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 449: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.1555e-04 - mean_absolute_error: 0.0166 - val_loss: 3.3959e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 450/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3110e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 450: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.3110e-04 - mean_absolute_error: 0.0167 - val_loss: 3.3475e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 451/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3863e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 451: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.3863e-04 - mean_absolute_error: 0.0170 - val_loss: 3.8279e-04 - val_mean_absolute_error: 0.0135\n",
      "Epoch 452/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2220e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 452: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.2220e-04 - mean_absolute_error: 0.0167 - val_loss: 3.1572e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 453/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1964e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 453: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 397ms/step - loss: 4.1964e-04 - mean_absolute_error: 0.0167 - val_loss: 3.2120e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 454/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3291e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 454: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 387ms/step - loss: 4.3291e-04 - mean_absolute_error: 0.0169 - val_loss: 3.7340e-04 - val_mean_absolute_error: 0.0136\n",
      "Epoch 455/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4362e-04 - mean_absolute_error: 0.0173\n",
      "Epoch 455: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 403ms/step - loss: 4.4362e-04 - mean_absolute_error: 0.0173 - val_loss: 3.3560e-04 - val_mean_absolute_error: 0.0140\n",
      "Epoch 456/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2807e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 456: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 36s 421ms/step - loss: 4.2807e-04 - mean_absolute_error: 0.0168 - val_loss: 3.6393e-04 - val_mean_absolute_error: 0.0127\n",
      "Epoch 457/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0359e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 457: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.0359e-04 - mean_absolute_error: 0.0165 - val_loss: 3.3196e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 458/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.5393e-04 - mean_absolute_error: 0.0172\n",
      "Epoch 458: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 409ms/step - loss: 4.5393e-04 - mean_absolute_error: 0.0172 - val_loss: 3.6300e-04 - val_mean_absolute_error: 0.0128\n",
      "Epoch 459/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2756e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 459: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 36s 423ms/step - loss: 4.2756e-04 - mean_absolute_error: 0.0166 - val_loss: 3.2397e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 460/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3091e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 460: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 414ms/step - loss: 4.3091e-04 - mean_absolute_error: 0.0170 - val_loss: 3.3957e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 461/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1662e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 461: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 406ms/step - loss: 4.1662e-04 - mean_absolute_error: 0.0167 - val_loss: 3.2435e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 462/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3934e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 462: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 36s 423ms/step - loss: 4.3934e-04 - mean_absolute_error: 0.0171 - val_loss: 3.3191e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 463/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3992e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 463: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 409ms/step - loss: 4.3992e-04 - mean_absolute_error: 0.0169 - val_loss: 4.3599e-04 - val_mean_absolute_error: 0.0139\n",
      "Epoch 464/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3502e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 464: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 389ms/step - loss: 4.3502e-04 - mean_absolute_error: 0.0167 - val_loss: 3.5685e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 465/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2869e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 465: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 392ms/step - loss: 4.2869e-04 - mean_absolute_error: 0.0168 - val_loss: 4.1063e-04 - val_mean_absolute_error: 0.0143\n",
      "Epoch 466/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4903e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 466: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 413ms/step - loss: 4.4903e-04 - mean_absolute_error: 0.0171 - val_loss: 3.1956e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 467/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3698e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 467: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 389ms/step - loss: 4.3698e-04 - mean_absolute_error: 0.0170 - val_loss: 3.1871e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 468/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2581e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 468: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 383ms/step - loss: 4.2581e-04 - mean_absolute_error: 0.0167 - val_loss: 3.2029e-04 - val_mean_absolute_error: 0.0120\n",
      "Epoch 469/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1535e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 469: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.1535e-04 - mean_absolute_error: 0.0167 - val_loss: 3.2145e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 470/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1822e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 470: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 36s 417ms/step - loss: 4.1822e-04 - mean_absolute_error: 0.0166 - val_loss: 3.1745e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 471/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3444e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 471: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 383ms/step - loss: 4.3444e-04 - mean_absolute_error: 0.0168 - val_loss: 3.5712e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 472/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3164e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 472: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 32s 382ms/step - loss: 4.3164e-04 - mean_absolute_error: 0.0165 - val_loss: 3.1760e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 473/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3091e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 473: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 396ms/step - loss: 4.3091e-04 - mean_absolute_error: 0.0168 - val_loss: 3.3839e-04 - val_mean_absolute_error: 0.0123\n",
      "Epoch 474/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4316e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 474: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.4316e-04 - mean_absolute_error: 0.0169 - val_loss: 3.2066e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 475/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2639e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 475: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 32s 377ms/step - loss: 4.2639e-04 - mean_absolute_error: 0.0169 - val_loss: 3.2161e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 476/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4343e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 476: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 387ms/step - loss: 4.4343e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2338e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 477/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3141e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 477: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.3141e-04 - mean_absolute_error: 0.0169 - val_loss: 3.4415e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 478/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2775e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 478: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 32s 381ms/step - loss: 4.2775e-04 - mean_absolute_error: 0.0169 - val_loss: 3.2775e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 479/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3389e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 479: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 32s 378ms/step - loss: 4.3389e-04 - mean_absolute_error: 0.0169 - val_loss: 3.6167e-04 - val_mean_absolute_error: 0.0129\n",
      "Epoch 480/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2459e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 480: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 392ms/step - loss: 4.2459e-04 - mean_absolute_error: 0.0166 - val_loss: 3.2202e-04 - val_mean_absolute_error: 0.0113\n",
      "Epoch 481/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0981e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 481: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 396ms/step - loss: 4.0981e-04 - mean_absolute_error: 0.0166 - val_loss: 3.2225e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 482/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4049e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 482: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 32s 380ms/step - loss: 4.4049e-04 - mean_absolute_error: 0.0170 - val_loss: 3.1531e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 483/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3838e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 483: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 416ms/step - loss: 4.3838e-04 - mean_absolute_error: 0.0168 - val_loss: 3.2825e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 484/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4880e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 484: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 401ms/step - loss: 4.4880e-04 - mean_absolute_error: 0.0171 - val_loss: 3.3166e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 485/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3896e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 485: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 391ms/step - loss: 4.3896e-04 - mean_absolute_error: 0.0168 - val_loss: 3.1510e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 486/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0770e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 486: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 394ms/step - loss: 4.0770e-04 - mean_absolute_error: 0.0164 - val_loss: 3.3008e-04 - val_mean_absolute_error: 0.0121\n",
      "Epoch 487/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1784e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 487: val_loss improved from 0.00031 to 0.00031, saving model to results\\2024-08-15_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "85/85 [==============================] - 35s 413ms/step - loss: 4.1784e-04 - mean_absolute_error: 0.0167 - val_loss: 3.1228e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 488/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1158e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 488: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 408ms/step - loss: 4.1158e-04 - mean_absolute_error: 0.0164 - val_loss: 3.2174e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 489/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2489e-04 - mean_absolute_error: 0.0168\n",
      "Epoch 489: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 396ms/step - loss: 4.2489e-04 - mean_absolute_error: 0.0168 - val_loss: 3.2017e-04 - val_mean_absolute_error: 0.0130\n",
      "Epoch 490/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1542e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 490: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 395ms/step - loss: 4.1542e-04 - mean_absolute_error: 0.0166 - val_loss: 3.2437e-04 - val_mean_absolute_error: 0.0117\n",
      "Epoch 491/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.1237e-04 - mean_absolute_error: 0.0166\n",
      "Epoch 491: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 414ms/step - loss: 4.1237e-04 - mean_absolute_error: 0.0166 - val_loss: 4.8904e-04 - val_mean_absolute_error: 0.0152\n",
      "Epoch 492/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3650e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 492: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 401ms/step - loss: 4.3650e-04 - mean_absolute_error: 0.0171 - val_loss: 3.8671e-04 - val_mean_absolute_error: 0.0132\n",
      "Epoch 493/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0535e-04 - mean_absolute_error: 0.0164\n",
      "Epoch 493: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.0535e-04 - mean_absolute_error: 0.0164 - val_loss: 3.1738e-04 - val_mean_absolute_error: 0.0124\n",
      "Epoch 494/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4822e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 494: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 417ms/step - loss: 4.4822e-04 - mean_absolute_error: 0.0171 - val_loss: 3.2546e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 495/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.4315e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 495: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 413ms/step - loss: 4.4315e-04 - mean_absolute_error: 0.0170 - val_loss: 3.1704e-04 - val_mean_absolute_error: 0.0115\n",
      "Epoch 496/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2885e-04 - mean_absolute_error: 0.0170\n",
      "Epoch 496: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 398ms/step - loss: 4.2885e-04 - mean_absolute_error: 0.0170 - val_loss: 3.6428e-04 - val_mean_absolute_error: 0.0148\n",
      "Epoch 497/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.3033e-04 - mean_absolute_error: 0.0171\n",
      "Epoch 497: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 393ms/step - loss: 4.3033e-04 - mean_absolute_error: 0.0171 - val_loss: 3.3925e-04 - val_mean_absolute_error: 0.0125\n",
      "Epoch 498/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.0818e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 498: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 35s 407ms/step - loss: 4.0818e-04 - mean_absolute_error: 0.0163 - val_loss: 3.2108e-04 - val_mean_absolute_error: 0.0116\n",
      "Epoch 499/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2110e-04 - mean_absolute_error: 0.0167\n",
      "Epoch 499: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 34s 400ms/step - loss: 4.2110e-04 - mean_absolute_error: 0.0167 - val_loss: 3.2218e-04 - val_mean_absolute_error: 0.0122\n",
      "Epoch 500/500\n",
      "85/85 [==============================] - ETA: 0s - loss: 4.2793e-04 - mean_absolute_error: 0.0169\n",
      "Epoch 500: val_loss did not improve from 0.00031\n",
      "85/85 [==============================] - 33s 394ms/step - loss: 4.2793e-04 - mean_absolute_error: 0.0169 - val_loss: 3.6204e-04 - val_mean_absolute_error: 0.0143\n"
     ]
    }
   ],
   "source": [
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 5s 91ms/step\n"
     ]
    }
   ],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 15 days is 175.11$\n",
      "huber_loss loss: 0.00031228194711729884\n",
      "Mean Absolute Error: 2.366551674816032\n",
      "Accuracy score: 0.4856512141280353\n",
      "Total buy profit: 670.9911272525787\n",
      "Total sell profit: 27.083147048950202\n",
      "Total profit: 698.0742743015289\n",
      "Profit per trade: 0.5136676043425525\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot true/pred prices graph\n",
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adjclose_15</th>\n",
       "      <th>true_adjclose_15</th>\n",
       "      <th>buy_profit</th>\n",
       "      <th>sell_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1997-07-29</th>\n",
       "      <td>0.118229</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.123958</td>\n",
       "      <td>0.123958</td>\n",
       "      <td>96288000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-07</th>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.113021</td>\n",
       "      <td>0.106250</td>\n",
       "      <td>0.108854</td>\n",
       "      <td>0.108854</td>\n",
       "      <td>40680000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.093506</td>\n",
       "      <td>0.118750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-08-21</th>\n",
       "      <td>0.106771</td>\n",
       "      <td>0.108594</td>\n",
       "      <td>0.103646</td>\n",
       "      <td>0.105729</td>\n",
       "      <td>0.105729</td>\n",
       "      <td>12480000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.092061</td>\n",
       "      <td>0.184375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.078646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-03</th>\n",
       "      <td>0.117188</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>0.115625</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>39288000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.064575</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.102083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-05</th>\n",
       "      <td>0.129167</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.122917</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>38160000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.069884</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-08</th>\n",
       "      <td>0.126563</td>\n",
       "      <td>0.151042</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>112968000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.049513</td>\n",
       "      <td>0.202083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.052083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-10</th>\n",
       "      <td>0.165625</td>\n",
       "      <td>0.166406</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.165104</td>\n",
       "      <td>0.165104</td>\n",
       "      <td>77328000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.030267</td>\n",
       "      <td>0.201042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.035938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-15</th>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.183854</td>\n",
       "      <td>0.152604</td>\n",
       "      <td>0.154688</td>\n",
       "      <td>0.154688</td>\n",
       "      <td>111672000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.031786</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.051562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-16</th>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.155990</td>\n",
       "      <td>0.167708</td>\n",
       "      <td>0.167708</td>\n",
       "      <td>128640000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>-0.001044</td>\n",
       "      <td>0.202865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.035157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-17</th>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.170313</td>\n",
       "      <td>0.170313</td>\n",
       "      <td>52152000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.121019</td>\n",
       "      <td>0.200260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.029947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-09-29</th>\n",
       "      <td>0.207292</td>\n",
       "      <td>0.209375</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.202083</td>\n",
       "      <td>0.202083</td>\n",
       "      <td>47424000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.182185</td>\n",
       "      <td>0.191146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-10-06</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.197135</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>40560000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.232894</td>\n",
       "      <td>0.213542</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-10-17</th>\n",
       "      <td>0.180729</td>\n",
       "      <td>0.182813</td>\n",
       "      <td>0.176042</td>\n",
       "      <td>0.181250</td>\n",
       "      <td>0.181250</td>\n",
       "      <td>50688000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.223958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.042708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-10-21</th>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.221875</td>\n",
       "      <td>0.192708</td>\n",
       "      <td>0.221354</td>\n",
       "      <td>0.221354</td>\n",
       "      <td>241920000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.012460</td>\n",
       "      <td>0.197396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-11-05</th>\n",
       "      <td>0.248958</td>\n",
       "      <td>0.255990</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>61872000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.211584</td>\n",
       "      <td>0.213021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-11-17</th>\n",
       "      <td>0.211458</td>\n",
       "      <td>0.227083</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>147888000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.089982</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-12-03</th>\n",
       "      <td>0.209896</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.209375</td>\n",
       "      <td>0.218229</td>\n",
       "      <td>0.218229</td>\n",
       "      <td>25800000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>0.230208</td>\n",
       "      <td>0.011979</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-12-12</th>\n",
       "      <td>0.232292</td>\n",
       "      <td>0.240104</td>\n",
       "      <td>0.220313</td>\n",
       "      <td>0.227083</td>\n",
       "      <td>0.227083</td>\n",
       "      <td>53448000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.130052</td>\n",
       "      <td>0.241927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.014844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-12-16</th>\n",
       "      <td>0.232292</td>\n",
       "      <td>0.232292</td>\n",
       "      <td>0.222917</td>\n",
       "      <td>0.222917</td>\n",
       "      <td>0.222917</td>\n",
       "      <td>33744000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.214679</td>\n",
       "      <td>0.230729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997-12-18</th>\n",
       "      <td>0.217708</td>\n",
       "      <td>0.221354</td>\n",
       "      <td>0.211458</td>\n",
       "      <td>0.214583</td>\n",
       "      <td>0.214583</td>\n",
       "      <td>64080000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.111441</td>\n",
       "      <td>0.215104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                open      high       low     close  adjclose     volume  \\\n",
       "1997-07-29  0.118229  0.125000  0.116667  0.123958  0.123958   96288000   \n",
       "1997-08-07  0.112500  0.113021  0.106250  0.108854  0.108854   40680000   \n",
       "1997-08-21  0.106771  0.108594  0.103646  0.105729  0.105729   12480000   \n",
       "1997-09-03  0.117188  0.120833  0.115625  0.116667  0.116667   39288000   \n",
       "1997-09-05  0.129167  0.133333  0.122917  0.125000  0.125000   38160000   \n",
       "1997-09-08  0.126563  0.151042  0.125000  0.150000  0.150000  112968000   \n",
       "1997-09-10  0.165625  0.166406  0.156250  0.165104  0.165104   77328000   \n",
       "1997-09-15  0.183333  0.183854  0.152604  0.154688  0.154688  111672000   \n",
       "1997-09-16  0.156250  0.177083  0.155990  0.167708  0.167708  128640000   \n",
       "1997-09-17  0.172917  0.175000  0.166667  0.170313  0.170313   52152000   \n",
       "1997-09-29  0.207292  0.209375  0.197917  0.202083  0.202083   47424000   \n",
       "1997-10-06  0.200000  0.206250  0.197135  0.206250  0.206250   40560000   \n",
       "1997-10-17  0.180729  0.182813  0.176042  0.181250  0.181250   50688000   \n",
       "1997-10-21  0.197917  0.221875  0.192708  0.221354  0.221354  241920000   \n",
       "1997-11-05  0.248958  0.255990  0.243750  0.243750  0.243750   61872000   \n",
       "1997-11-17  0.211458  0.227083  0.210938  0.218750  0.218750  147888000   \n",
       "1997-12-03  0.209896  0.218750  0.209375  0.218229  0.218229   25800000   \n",
       "1997-12-12  0.232292  0.240104  0.220313  0.227083  0.227083   53448000   \n",
       "1997-12-16  0.232292  0.232292  0.222917  0.222917  0.222917   33744000   \n",
       "1997-12-18  0.217708  0.221354  0.211458  0.214583  0.214583   64080000   \n",
       "\n",
       "           ticker  adjclose_15  true_adjclose_15  buy_profit  sell_profit  \n",
       "1997-07-29   AMZN     0.023105          0.108333    0.000000     0.015625  \n",
       "1997-08-07   AMZN     0.093506          0.118750    0.000000    -0.009896  \n",
       "1997-08-21   AMZN     0.092061          0.184375    0.000000    -0.078646  \n",
       "1997-09-03   AMZN     0.064575          0.218750    0.000000    -0.102083  \n",
       "1997-09-05   AMZN     0.069884          0.208333    0.000000    -0.083333  \n",
       "1997-09-08   AMZN     0.049513          0.202083    0.000000    -0.052083  \n",
       "1997-09-10   AMZN     0.030267          0.201042    0.000000    -0.035938  \n",
       "1997-09-15   AMZN     0.031786          0.206250    0.000000    -0.051562  \n",
       "1997-09-16   AMZN    -0.001044          0.202865    0.000000    -0.035157  \n",
       "1997-09-17   AMZN     0.121019          0.200260    0.000000    -0.029947  \n",
       "1997-09-29   AMZN     0.182185          0.191146    0.000000     0.010937  \n",
       "1997-10-06   AMZN     0.232894          0.213542    0.007292     0.000000  \n",
       "1997-10-17   AMZN     0.118800          0.223958    0.000000    -0.042708  \n",
       "1997-10-21   AMZN     0.012460          0.197396    0.000000     0.023958  \n",
       "1997-11-05   AMZN     0.211584          0.213021    0.000000     0.030729  \n",
       "1997-11-17   AMZN     0.089982          0.234375    0.000000    -0.015625  \n",
       "1997-12-03   AMZN     0.218579          0.230208    0.011979     0.000000  \n",
       "1997-12-12   AMZN     0.130052          0.241927    0.000000    -0.014844  \n",
       "1997-12-16   AMZN     0.214679          0.230729    0.000000    -0.007812  \n",
       "1997-12-18   AMZN     0.111441          0.215104    0.000000    -0.000521  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adjclose_15</th>\n",
       "      <th>true_adjclose_15</th>\n",
       "      <th>buy_profit</th>\n",
       "      <th>sell_profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-04-10</th>\n",
       "      <td>182.770004</td>\n",
       "      <td>186.270004</td>\n",
       "      <td>182.669998</td>\n",
       "      <td>185.949997</td>\n",
       "      <td>185.949997</td>\n",
       "      <td>35879200</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>178.518478</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.949997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-11</th>\n",
       "      <td>186.740005</td>\n",
       "      <td>189.770004</td>\n",
       "      <td>185.509995</td>\n",
       "      <td>189.050003</td>\n",
       "      <td>189.050003</td>\n",
       "      <td>40020700</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>179.290131</td>\n",
       "      <td>184.720001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.330002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-19</th>\n",
       "      <td>178.740005</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>173.440002</td>\n",
       "      <td>174.630005</td>\n",
       "      <td>174.630005</td>\n",
       "      <td>55950000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>174.833466</td>\n",
       "      <td>187.479996</td>\n",
       "      <td>12.849991</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-23</th>\n",
       "      <td>178.080002</td>\n",
       "      <td>179.929993</td>\n",
       "      <td>175.979996</td>\n",
       "      <td>179.539993</td>\n",
       "      <td>179.539993</td>\n",
       "      <td>37046500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>175.410400</td>\n",
       "      <td>187.070007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.530014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-25</th>\n",
       "      <td>169.679993</td>\n",
       "      <td>173.919998</td>\n",
       "      <td>166.320007</td>\n",
       "      <td>173.669998</td>\n",
       "      <td>173.669998</td>\n",
       "      <td>49249400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>173.080231</td>\n",
       "      <td>183.630005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.960007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-26</th>\n",
       "      <td>177.800003</td>\n",
       "      <td>180.820007</td>\n",
       "      <td>176.130005</td>\n",
       "      <td>179.619995</td>\n",
       "      <td>179.619995</td>\n",
       "      <td>43919800</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>175.326172</td>\n",
       "      <td>184.699997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.080002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-29</th>\n",
       "      <td>182.750000</td>\n",
       "      <td>183.529999</td>\n",
       "      <td>179.389999</td>\n",
       "      <td>180.960007</td>\n",
       "      <td>180.960007</td>\n",
       "      <td>54063900</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>177.319763</td>\n",
       "      <td>183.539993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.579987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-02</th>\n",
       "      <td>180.850006</td>\n",
       "      <td>185.100006</td>\n",
       "      <td>179.910004</td>\n",
       "      <td>184.720001</td>\n",
       "      <td>184.720001</td>\n",
       "      <td>54303500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>178.368835</td>\n",
       "      <td>181.050003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.669998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-08</th>\n",
       "      <td>187.440002</td>\n",
       "      <td>188.429993</td>\n",
       "      <td>186.389999</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>26136400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>180.629974</td>\n",
       "      <td>179.320007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.679993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-09</th>\n",
       "      <td>188.880005</td>\n",
       "      <td>191.699997</td>\n",
       "      <td>187.440002</td>\n",
       "      <td>189.500000</td>\n",
       "      <td>189.500000</td>\n",
       "      <td>43368400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>180.826340</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.059998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-13</th>\n",
       "      <td>188.000000</td>\n",
       "      <td>188.309998</td>\n",
       "      <td>185.360001</td>\n",
       "      <td>186.570007</td>\n",
       "      <td>186.570007</td>\n",
       "      <td>24898600</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>180.019745</td>\n",
       "      <td>179.339996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.230011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-28</th>\n",
       "      <td>179.929993</td>\n",
       "      <td>182.240005</td>\n",
       "      <td>179.490005</td>\n",
       "      <td>182.149994</td>\n",
       "      <td>182.149994</td>\n",
       "      <td>29927000</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>177.868103</td>\n",
       "      <td>182.809998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.660004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-05</th>\n",
       "      <td>180.100006</td>\n",
       "      <td>181.500000</td>\n",
       "      <td>178.750000</td>\n",
       "      <td>181.279999</td>\n",
       "      <td>181.279999</td>\n",
       "      <td>32116400</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>177.772888</td>\n",
       "      <td>197.850006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16.570007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-07</th>\n",
       "      <td>184.899994</td>\n",
       "      <td>186.289993</td>\n",
       "      <td>183.360001</td>\n",
       "      <td>184.300003</td>\n",
       "      <td>184.300003</td>\n",
       "      <td>28021500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>179.789719</td>\n",
       "      <td>197.199997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-12.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-10</th>\n",
       "      <td>184.070007</td>\n",
       "      <td>187.229996</td>\n",
       "      <td>183.789993</td>\n",
       "      <td>187.059998</td>\n",
       "      <td>187.059998</td>\n",
       "      <td>34494500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>180.291428</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-12.940002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-13</th>\n",
       "      <td>186.089996</td>\n",
       "      <td>187.669998</td>\n",
       "      <td>182.669998</td>\n",
       "      <td>183.830002</td>\n",
       "      <td>183.830002</td>\n",
       "      <td>39721500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>179.932922</td>\n",
       "      <td>199.289993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15.459991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-27</th>\n",
       "      <td>195.009995</td>\n",
       "      <td>199.839996</td>\n",
       "      <td>194.199997</td>\n",
       "      <td>197.850006</td>\n",
       "      <td>197.850006</td>\n",
       "      <td>74397500</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>182.712814</td>\n",
       "      <td>183.130005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.720001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-28</th>\n",
       "      <td>197.729996</td>\n",
       "      <td>198.850006</td>\n",
       "      <td>192.500000</td>\n",
       "      <td>193.250000</td>\n",
       "      <td>193.250000</td>\n",
       "      <td>76930200</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>182.673431</td>\n",
       "      <td>182.550003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.699997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-08</th>\n",
       "      <td>200.039993</td>\n",
       "      <td>201.199997</td>\n",
       "      <td>197.960007</td>\n",
       "      <td>199.289993</td>\n",
       "      <td>199.289993</td>\n",
       "      <td>34767300</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>183.086624</td>\n",
       "      <td>183.199997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.089996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-17</th>\n",
       "      <td>191.350006</td>\n",
       "      <td>191.580002</td>\n",
       "      <td>185.990005</td>\n",
       "      <td>187.929993</td>\n",
       "      <td>187.929993</td>\n",
       "      <td>48076100</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>180.734833</td>\n",
       "      <td>162.770004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.159988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  open        high         low       close    adjclose  \\\n",
       "2024-04-10  182.770004  186.270004  182.669998  185.949997  185.949997   \n",
       "2024-04-11  186.740005  189.770004  185.509995  189.050003  189.050003   \n",
       "2024-04-19  178.740005  179.000000  173.440002  174.630005  174.630005   \n",
       "2024-04-23  178.080002  179.929993  175.979996  179.539993  179.539993   \n",
       "2024-04-25  169.679993  173.919998  166.320007  173.669998  173.669998   \n",
       "2024-04-26  177.800003  180.820007  176.130005  179.619995  179.619995   \n",
       "2024-04-29  182.750000  183.529999  179.389999  180.960007  180.960007   \n",
       "2024-05-02  180.850006  185.100006  179.910004  184.720001  184.720001   \n",
       "2024-05-08  187.440002  188.429993  186.389999  188.000000  188.000000   \n",
       "2024-05-09  188.880005  191.699997  187.440002  189.500000  189.500000   \n",
       "2024-05-13  188.000000  188.309998  185.360001  186.570007  186.570007   \n",
       "2024-05-28  179.929993  182.240005  179.490005  182.149994  182.149994   \n",
       "2024-06-05  180.100006  181.500000  178.750000  181.279999  181.279999   \n",
       "2024-06-07  184.899994  186.289993  183.360001  184.300003  184.300003   \n",
       "2024-06-10  184.070007  187.229996  183.789993  187.059998  187.059998   \n",
       "2024-06-13  186.089996  187.669998  182.669998  183.830002  183.830002   \n",
       "2024-06-27  195.009995  199.839996  194.199997  197.850006  197.850006   \n",
       "2024-06-28  197.729996  198.850006  192.500000  193.250000  193.250000   \n",
       "2024-07-08  200.039993  201.199997  197.960007  199.289993  199.289993   \n",
       "2024-07-17  191.350006  191.580002  185.990005  187.929993  187.929993   \n",
       "\n",
       "              volume ticker  adjclose_15  true_adjclose_15  buy_profit  \\\n",
       "2024-04-10  35879200   AMZN   178.518478        179.000000    0.000000   \n",
       "2024-04-11  40020700   AMZN   179.290131        184.720001    0.000000   \n",
       "2024-04-19  55950000   AMZN   174.833466        187.479996   12.849991   \n",
       "2024-04-23  37046500   AMZN   175.410400        187.070007    0.000000   \n",
       "2024-04-25  49249400   AMZN   173.080231        183.630005    0.000000   \n",
       "2024-04-26  43919800   AMZN   175.326172        184.699997    0.000000   \n",
       "2024-04-29  54063900   AMZN   177.319763        183.539993    0.000000   \n",
       "2024-05-02  54303500   AMZN   178.368835        181.050003    0.000000   \n",
       "2024-05-08  26136400   AMZN   180.629974        179.320007    0.000000   \n",
       "2024-05-09  43368400   AMZN   180.826340        176.440002    0.000000   \n",
       "2024-05-13  24898600   AMZN   180.019745        179.339996    0.000000   \n",
       "2024-05-28  29927000   AMZN   177.868103        182.809998    0.000000   \n",
       "2024-06-05  32116400   AMZN   177.772888        197.850006    0.000000   \n",
       "2024-06-07  28021500   AMZN   179.789719        197.199997    0.000000   \n",
       "2024-06-10  34494500   AMZN   180.291428        200.000000    0.000000   \n",
       "2024-06-13  39721500   AMZN   179.932922        199.289993    0.000000   \n",
       "2024-06-27  74397500   AMZN   182.712814        183.130005    0.000000   \n",
       "2024-06-28  76930200   AMZN   182.673431        182.550003    0.000000   \n",
       "2024-07-08  34767300   AMZN   183.086624        183.199997    0.000000   \n",
       "2024-07-17  48076100   AMZN   180.734833        162.770004    0.000000   \n",
       "\n",
       "            sell_profit  \n",
       "2024-04-10     6.949997  \n",
       "2024-04-11     4.330002  \n",
       "2024-04-19     0.000000  \n",
       "2024-04-23    -7.530014  \n",
       "2024-04-25    -9.960007  \n",
       "2024-04-26    -5.080002  \n",
       "2024-04-29    -2.579987  \n",
       "2024-05-02     3.669998  \n",
       "2024-05-08     8.679993  \n",
       "2024-05-09    13.059998  \n",
       "2024-05-13     7.230011  \n",
       "2024-05-28    -0.660004  \n",
       "2024-06-05   -16.570007  \n",
       "2024-06-07   -12.899994  \n",
       "2024-06-10   -12.940002  \n",
       "2024-06-13   -15.459991  \n",
       "2024-06-27    14.720001  \n",
       "2024-06-28    10.699997  \n",
       "2024-07-08    16.089996  \n",
       "2024-07-17    25.159988  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final dataframe to csv-results folder\n",
    "csv_results_folder = \"csv-results\"\n",
    "if not os.path.isdir(csv_results_folder):\n",
    "    os.mkdir(csv_results_folder)\n",
    "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
    "final_df.to_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
